{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinajahangir/HydrologicalDL_Colab/blob/main/KAN_CONUS_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZW62R__B8f"
      },
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOELG5gvioA0",
        "outputId": "878250c2-82b0-434a-8148-14207803a6db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-decision-forests\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests) (2.0.3)\n",
            "Collecting tensorflow~=2.16.1 (from tensorflow-decision-forests)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests)\n",
            "  Downloading wurlitzer-3.1.0-py3-none-any.whl (8.4 kB)\n",
            "Collecting tf-keras~=2.16 (from tensorflow-decision-forests)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow-decision-forests)\n",
            "  Downloading ydf-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow~=2.16.1->tensorflow-decision-forests)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.16.1->tensorflow-decision-forests)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (1.64.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow~=2.16.1->tensorflow-decision-forests)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow~=2.16.1->tensorflow-decision-forests)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.16.1->tensorflow-decision-forests) (0.37.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests) (2024.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow~=2.16.1->tensorflow-decision-forests) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow~=2.16.1->tensorflow-decision-forests)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow~=2.16.1->tensorflow-decision-forests)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow-decision-forests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow-decision-forests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow-decision-forests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow~=2.16.1->tensorflow-decision-forests) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow-decision-forests) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow-decision-forests) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow-decision-forests) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tensorflow-decision-forests) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tensorflow-decision-forests) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tensorflow-decision-forests) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow~=2.16.1->tensorflow-decision-forests) (0.1.2)\n",
            "Installing collected packages: namex, ydf, wurlitzer, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow-decision-forests\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-decision-forests-1.9.1 tf-keras-2.16.0 wurlitzer-3.1.0 ydf-0.4.3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install tensorflow-decision-forests\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viKq13Omn013",
        "outputId": "ea1c1e96-a382-4209-e5ca-c9a28bbc537a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raZEQshMqalU"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED_QC82CqwsM"
      },
      "outputs": [],
      "source": [
        "csv_path_train='/content/drive/MyDrive/PHIMP/All421_TrainTest/All421data_Train_v1.csv'\n",
        "csv_path_test='/content/drive/MyDrive/PHIMP/All421_TrainTest/All421data_Test_v1.csv'\n",
        "df_train = pd.read_csv(csv_path_train)\n",
        "df_test = pd.read_csv(csv_path_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "83jg4Ttsgl39",
        "outputId": "0a727631-5220-4f0c-de59-38d354770a59"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "0",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6be907b9-2c94-4668-92e7-0434929f6af8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>pr</th>\n",
              "      <th>srad</th>\n",
              "      <th>tmax</th>\n",
              "      <th>tmin</th>\n",
              "      <th>vp</th>\n",
              "      <th>q</th>\n",
              "      <th>basin_id</th>\n",
              "      <th>average_pr</th>\n",
              "      <th>average_q</th>\n",
              "      <th>average_tmax</th>\n",
              "      <th>average_tmin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3608807</th>\n",
              "      <td>2007-11-20</td>\n",
              "      <td>6.6</td>\n",
              "      <td>152.75</td>\n",
              "      <td>7.10</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>536.88</td>\n",
              "      <td>27.62</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3608808</th>\n",
              "      <td>2007-11-21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>232.52</td>\n",
              "      <td>11.45</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>608.75</td>\n",
              "      <td>17.68</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3608809</th>\n",
              "      <td>2007-11-22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>228.97</td>\n",
              "      <td>11.89</td>\n",
              "      <td>0.45</td>\n",
              "      <td>629.82</td>\n",
              "      <td>12.73</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3608810</th>\n",
              "      <td>2007-11-23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>244.66</td>\n",
              "      <td>15.05</td>\n",
              "      <td>0.48</td>\n",
              "      <td>629.72</td>\n",
              "      <td>9.90</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3608811</th>\n",
              "      <td>2007-11-24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>191.25</td>\n",
              "      <td>11.66</td>\n",
              "      <td>2.91</td>\n",
              "      <td>752.85</td>\n",
              "      <td>8.09</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6be907b9-2c94-4668-92e7-0434929f6af8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6be907b9-2c94-4668-92e7-0434929f6af8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6be907b9-2c94-4668-92e7-0434929f6af8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5adbd082-c1ba-4a71-86a0-4399822e0a92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5adbd082-c1ba-4a71-86a0-4399822e0a92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5adbd082-c1ba-4a71-86a0-4399822e0a92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "               date   pr    srad   tmax  tmin      vp      q  basin_id  \\\n",
              "3608807  2007-11-20  6.6  152.75   7.10 -1.82  536.88  27.62       420   \n",
              "3608808  2007-11-21  0.0  232.52  11.45 -0.07  608.75  17.68       420   \n",
              "3608809  2007-11-22  0.0  228.97  11.89  0.45  629.82  12.73       420   \n",
              "3608810  2007-11-23  0.0  244.66  15.05  0.48  629.72   9.90       420   \n",
              "3608811  2007-11-24  0.0  191.25  11.66  2.91  752.85   8.09       420   \n",
              "\n",
              "         average_pr  average_q  average_tmax  average_tmin  \n",
              "3608807    5.450285   7.414476     14.486049      4.866612  \n",
              "3608808    5.450285   7.414476     14.486049      4.866612  \n",
              "3608809    5.450285   7.414476     14.486049      4.866612  \n",
              "3608810    5.450285   7.414476     14.486049      4.866612  \n",
              "3608811    5.450285   7.414476     14.486049      4.866612  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "TL9af73QFk_Q",
        "outputId": "6a31c56e-9b90-48df-d803-20f68ea2dd84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "0",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b6a9f81c-16cd-4eed-a40a-a33bcf2c85f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>pr</th>\n",
              "      <th>srad</th>\n",
              "      <th>tmax</th>\n",
              "      <th>tmin</th>\n",
              "      <th>vp</th>\n",
              "      <th>q</th>\n",
              "      <th>basin_id</th>\n",
              "      <th>average_pr</th>\n",
              "      <th>average_q</th>\n",
              "      <th>average_tmax</th>\n",
              "      <th>average_tmin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>636968</th>\n",
              "      <td>2012-01-11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>188.99</td>\n",
              "      <td>11.73</td>\n",
              "      <td>3.85</td>\n",
              "      <td>804.71</td>\n",
              "      <td>4.53</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636969</th>\n",
              "      <td>2012-01-12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>225.65</td>\n",
              "      <td>13.29</td>\n",
              "      <td>2.11</td>\n",
              "      <td>714.24</td>\n",
              "      <td>4.32</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636970</th>\n",
              "      <td>2012-01-13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>233.53</td>\n",
              "      <td>14.33</td>\n",
              "      <td>2.02</td>\n",
              "      <td>710.75</td>\n",
              "      <td>4.11</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636971</th>\n",
              "      <td>2012-01-14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>207.19</td>\n",
              "      <td>10.63</td>\n",
              "      <td>1.87</td>\n",
              "      <td>698.56</td>\n",
              "      <td>3.94</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636972</th>\n",
              "      <td>2012-01-15</td>\n",
              "      <td>9.07</td>\n",
              "      <td>106.43</td>\n",
              "      <td>3.53</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>546.38</td>\n",
              "      <td>3.84</td>\n",
              "      <td>420</td>\n",
              "      <td>5.450285</td>\n",
              "      <td>7.414476</td>\n",
              "      <td>14.486049</td>\n",
              "      <td>4.866612</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6a9f81c-16cd-4eed-a40a-a33bcf2c85f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b6a9f81c-16cd-4eed-a40a-a33bcf2c85f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b6a9f81c-16cd-4eed-a40a-a33bcf2c85f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4e8d0073-cd93-4646-8c76-52433afe878e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e8d0073-cd93-4646-8c76-52433afe878e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4e8d0073-cd93-4646-8c76-52433afe878e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "              date    pr    srad   tmax  tmin      vp     q  basin_id  \\\n",
              "636968  2012-01-11  0.00  188.99  11.73  3.85  804.71  4.53       420   \n",
              "636969  2012-01-12  0.00  225.65  13.29  2.11  714.24  4.32       420   \n",
              "636970  2012-01-13  0.00  233.53  14.33  2.02  710.75  4.11       420   \n",
              "636971  2012-01-14  0.00  207.19  10.63  1.87  698.56  3.94       420   \n",
              "636972  2012-01-15  9.07  106.43   3.53 -1.60  546.38  3.84       420   \n",
              "\n",
              "        average_pr  average_q  average_tmax  average_tmin  \n",
              "636968    5.450285   7.414476     14.486049      4.866612  \n",
              "636969    5.450285   7.414476     14.486049      4.866612  \n",
              "636970    5.450285   7.414476     14.486049      4.866612  \n",
              "636971    5.450285   7.414476     14.486049      4.866612  \n",
              "636972    5.450285   7.414476     14.486049      4.866612  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmeA5Y4uejC-"
      },
      "outputs": [],
      "source": [
        "def split_sequence_multi_train(sequence_x,sequence_y, n_steps_in, n_steps_out,mode='seq'):\n",
        "    \"\"\"\n",
        "    written by:SJ\n",
        "    sequence_x=features; 2D array\n",
        "    sequence_y=target; 2D array\n",
        "    n_steps_in=IL(lookbak period);int\n",
        "    n_steps_out=forecast horizon;int\n",
        "    mode:either single (many to one) or seq (many to many).\n",
        "    This function creates an output in shape of (sample,IL,feature) for x and\n",
        "    (sample,n_steps_out) for y\n",
        "    \"\"\"\n",
        "    X, y = list(), list()\n",
        "    k=0\n",
        "    sequence_x=np.copy(np.asarray(sequence_x))\n",
        "    sequence_y=np.copy(np.asarray(sequence_y))\n",
        "    for _ in range(len(sequence_x)):\n",
        "\t\t# find the end of this pattern\n",
        "        end_ix = k + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the sequence\n",
        "        if out_end_ix > len(sequence_x):\n",
        "            break\n",
        "\t\t# gather input and output parts of the pattern\n",
        "        seq_x = sequence_x[k:end_ix]\n",
        "        #mode single is used for one output\n",
        "        if n_steps_out==0:\n",
        "            seq_y= sequence_y[end_ix-1:out_end_ix]\n",
        "        elif mode=='single':\n",
        "            seq_y= sequence_y[out_end_ix-1]\n",
        "        else:\n",
        "            seq_y= sequence_y[end_ix:out_end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y.flatten())\n",
        "        k=k+1\n",
        "\n",
        "    XX,YY= np.asarray(X), np.asarray(y)\n",
        "    if (n_steps_out==0 or n_steps_out==1):\n",
        "        YY=YY.reshape((len(XX),1))\n",
        "    return XX,YY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rwE-9Cata0V"
      },
      "outputs": [],
      "source": [
        "mean_=np.asarray(df_train.iloc[:,1:].mean())\n",
        "std_=np.asarray(df_train.iloc[:,1:].std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDIUuKJCOBEQ"
      },
      "outputs": [],
      "source": [
        "df_test_tr=df_test.iloc[:,1:]-mean_\n",
        "df_test_tr=df_test_tr/std_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CdE_vhXEOP8W",
        "outputId": "2a273c59-7567-4a56-abdb-9d1acfc868d8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_test_tr"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2f945e96-c87c-435e-bcfd-3566982f7881\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr</th>\n",
              "      <th>srad</th>\n",
              "      <th>tmax</th>\n",
              "      <th>tmin</th>\n",
              "      <th>vp</th>\n",
              "      <th>q</th>\n",
              "      <th>basin_id</th>\n",
              "      <th>average_pr</th>\n",
              "      <th>average_q</th>\n",
              "      <th>average_tmax</th>\n",
              "      <th>average_tmin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.414810</td>\n",
              "      <td>-1.314938</td>\n",
              "      <td>-1.571585</td>\n",
              "      <td>-1.384286</td>\n",
              "      <td>-1.027523</td>\n",
              "      <td>0.060093</td>\n",
              "      <td>-1.727941</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.414810</td>\n",
              "      <td>-1.321855</td>\n",
              "      <td>-1.104175</td>\n",
              "      <td>-0.888873</td>\n",
              "      <td>-0.803962</td>\n",
              "      <td>0.019014</td>\n",
              "      <td>-1.727941</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.274691</td>\n",
              "      <td>-1.818817</td>\n",
              "      <td>-0.806566</td>\n",
              "      <td>-0.372484</td>\n",
              "      <td>-0.499100</td>\n",
              "      <td>0.312045</td>\n",
              "      <td>-1.727941</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.414810</td>\n",
              "      <td>-1.216401</td>\n",
              "      <td>-0.954457</td>\n",
              "      <td>-0.890870</td>\n",
              "      <td>-0.818998</td>\n",
              "      <td>0.542088</td>\n",
              "      <td>-1.727941</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.399464</td>\n",
              "      <td>-1.172727</td>\n",
              "      <td>-1.318709</td>\n",
              "      <td>-1.340338</td>\n",
              "      <td>-1.015989</td>\n",
              "      <td>0.459930</td>\n",
              "      <td>-1.727941</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f945e96-c87c-435e-bcfd-3566982f7881')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f945e96-c87c-435e-bcfd-3566982f7881 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f945e96-c87c-435e-bcfd-3566982f7881');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1fbd3af1-7944-4d72-ac93-ad554ec5e764\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1fbd3af1-7944-4d72-ac93-ad554ec5e764')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1fbd3af1-7944-4d72-ac93-ad554ec5e764 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         pr      srad      tmax      tmin        vp         q  basin_id  \\\n",
              "0 -0.414810 -1.314938 -1.571585 -1.384286 -1.027523  0.060093 -1.727941   \n",
              "1 -0.414810 -1.321855 -1.104175 -0.888873 -0.803962  0.019014 -1.727941   \n",
              "2  2.274691 -1.818817 -0.806566 -0.372484 -0.499100  0.312045 -1.727941   \n",
              "3 -0.414810 -1.216401 -0.954457 -0.890870 -0.818998  0.542088 -1.727941   \n",
              "4 -0.399464 -1.172727 -1.318709 -1.340338 -1.015989  0.459930 -1.727941   \n",
              "\n",
              "   average_pr  average_q  average_tmax  average_tmin  \n",
              "0    0.164944   0.316118     -0.951796     -0.764845  \n",
              "1    0.164944   0.316118     -0.951796     -0.764845  \n",
              "2    0.164944   0.316118     -0.951796     -0.764845  \n",
              "3    0.164944   0.316118     -0.951796     -0.764845  \n",
              "4    0.164944   0.316118     -0.951796     -0.764845  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_tr.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llnihM6KPcTY"
      },
      "outputs": [],
      "source": [
        "df_test_tr=df_test_tr.drop(columns=['basin_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOQ_NUaGRQmR"
      },
      "outputs": [],
      "source": [
        "# prompt: add the basin_id column of df_test to df_test_tr\n",
        "df_test_tr['basin_id'] = df_test['basin_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE5On7AwTRYP"
      },
      "outputs": [],
      "source": [
        "mean_q=df_train['q'].mean()\n",
        "std_q=df_train['q'].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "oeR5gCGXSXd9",
        "outputId": "2fdff97e-4628-455c-9af5-1f8849518c23"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_test_tr"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-909b2571-62f3-4a36-838f-d825ec7dcabf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr</th>\n",
              "      <th>srad</th>\n",
              "      <th>tmax</th>\n",
              "      <th>tmin</th>\n",
              "      <th>vp</th>\n",
              "      <th>q</th>\n",
              "      <th>average_pr</th>\n",
              "      <th>average_q</th>\n",
              "      <th>average_tmax</th>\n",
              "      <th>average_tmin</th>\n",
              "      <th>basin_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.414810</td>\n",
              "      <td>-1.314938</td>\n",
              "      <td>-1.571585</td>\n",
              "      <td>-1.384286</td>\n",
              "      <td>-1.027523</td>\n",
              "      <td>0.060093</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.414810</td>\n",
              "      <td>-1.321855</td>\n",
              "      <td>-1.104175</td>\n",
              "      <td>-0.888873</td>\n",
              "      <td>-0.803962</td>\n",
              "      <td>0.019014</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.274691</td>\n",
              "      <td>-1.818817</td>\n",
              "      <td>-0.806566</td>\n",
              "      <td>-0.372484</td>\n",
              "      <td>-0.499100</td>\n",
              "      <td>0.312045</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.414810</td>\n",
              "      <td>-1.216401</td>\n",
              "      <td>-0.954457</td>\n",
              "      <td>-0.890870</td>\n",
              "      <td>-0.818998</td>\n",
              "      <td>0.542088</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.399464</td>\n",
              "      <td>-1.172727</td>\n",
              "      <td>-1.318709</td>\n",
              "      <td>-1.340338</td>\n",
              "      <td>-1.015989</td>\n",
              "      <td>0.459930</td>\n",
              "      <td>0.164944</td>\n",
              "      <td>0.316118</td>\n",
              "      <td>-0.951796</td>\n",
              "      <td>-0.764845</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-909b2571-62f3-4a36-838f-d825ec7dcabf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-909b2571-62f3-4a36-838f-d825ec7dcabf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-909b2571-62f3-4a36-838f-d825ec7dcabf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9874bb7e-bde9-4fc0-b413-3b3607f5e687\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9874bb7e-bde9-4fc0-b413-3b3607f5e687')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9874bb7e-bde9-4fc0-b413-3b3607f5e687 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         pr      srad      tmax      tmin        vp         q  average_pr  \\\n",
              "0 -0.414810 -1.314938 -1.571585 -1.384286 -1.027523  0.060093    0.164944   \n",
              "1 -0.414810 -1.321855 -1.104175 -0.888873 -0.803962  0.019014    0.164944   \n",
              "2  2.274691 -1.818817 -0.806566 -0.372484 -0.499100  0.312045    0.164944   \n",
              "3 -0.414810 -1.216401 -0.954457 -0.890870 -0.818998  0.542088    0.164944   \n",
              "4 -0.399464 -1.172727 -1.318709 -1.340338 -1.015989  0.459930    0.164944   \n",
              "\n",
              "   average_q  average_tmax  average_tmin  basin_id  \n",
              "0   0.316118     -0.951796     -0.764845         0  \n",
              "1   0.316118     -0.951796     -0.764845         0  \n",
              "2   0.316118     -0.951796     -0.764845         0  \n",
              "3   0.316118     -0.951796     -0.764845         0  \n",
              "4   0.316118     -0.951796     -0.764845         0  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test_tr.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTCwUCe2SI1t"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.layers.Input(shape=(365, 9))\n",
        "#x=tf.keras.layers.Normalization(axis=-1)(inputs)\n",
        "x = tf.keras.layers.LSTM(256, return_sequences=False)(inputs)\n",
        "x=tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation='linear')(x)\n",
        "#outputs=tf.keras.layers.Reshape((3,1),name='output')(outputs)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3), loss=\"mse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9GyezYrRp6H"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4WpxQiuQncW"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ij11sHrSaLr"
      },
      "outputs": [],
      "source": [
        "def forecast_wrapper(array,n_steps_out=7):\n",
        "    \"\"\"\n",
        "    Written by:SJ\n",
        "    array:predictions;2D vector\n",
        "    n_steps_out:number of steps out;int\n",
        "    This function outputs a list with n_steps_out elements containing stepwise forecasts\n",
        "    returns:list of forecasts\n",
        "    \"\"\"\n",
        "    all_steps=[]\n",
        "    for ii in range(0,n_steps_out):\n",
        "        all_=array[ii:len(array):n_steps_out]\n",
        "        all_steps.append(all_[n_steps_out-1-ii:len(all_)-ii])\n",
        "    return all_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0yWsrmbTw5e"
      },
      "outputs": [],
      "source": [
        "# prompt: write nash-sutcliffe error index\n",
        "\n",
        "def nash_sutcliffe_error(Q_obs,Q_sim):\n",
        "    \"\"\"\n",
        "    Written by: SJ\n",
        "    Q_obs: observed discharge; 1D vector\n",
        "    Q_sim: simulated discharge; 1D vector\n",
        "    This function calculates the NSE between observed and simulated discharges\n",
        "    returns: NSE; float\n",
        "    \"\"\"\n",
        "    if len(Q_sim)!=len(Q_obs):\n",
        "        print('Length of simulated and observed discharges do not match')\n",
        "        return\n",
        "    else:\n",
        "        num=np.sum(np.square(Q_sim-Q_obs))\n",
        "        den=np.sum(np.square(Q_obs-np.mean(Q_obs)))\n",
        "        NSE=1-(num/den)\n",
        "        return NSE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY_WkoB_U6mU"
      },
      "outputs": [],
      "source": [
        "def i_normalize(x_tr,mean_,sd_):\n",
        "  \"\"\"\n",
        "  Written by:SJ\n",
        "  mean_,sd_,:mean_, and sd_ used for initial transformation; 2D array\n",
        "  x_tr:transformed input\n",
        "  This function inverses the transformation\n",
        "  returns:inverse transfomed\n",
        "  \"\"\"\n",
        "  x_i=x_tr*sd_+mean_\n",
        "  return x_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZPlWD_KCI41"
      },
      "outputs": [],
      "source": [
        "def CC(Pr,Y):\n",
        "    from scipy import stats\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    return stats.pearsonr(Pr.flatten(),Y.flatten())[0]\n",
        "def KGE(prediction,observation):\n",
        "\n",
        "    nas = np.logical_or(np.isnan(prediction), np.isnan(observation))\n",
        "    pred=np.copy(np.reshape(prediction,(-1,1)))\n",
        "    obs=np.copy(np.reshape(observation,(-1,1)))\n",
        "    r=CC(pred[~nas],obs[~nas])\n",
        "    beta=np.nanmean(pred)/np.nanmean(obs)\n",
        "    gamma=(np.nanstd(pred)/np.nanstd(obs))/beta\n",
        "    kge=1-((r-1)**2+(beta-1)**2+(gamma-1)**2)**0.5\n",
        "    return kge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dqHfmYcXUCcs"
      },
      "outputs": [],
      "source": [
        "nse_scores_same_lstm=[]\n",
        "kge_scores_same_lstm=[]\n",
        "\n",
        "nse_scores_diff_lstm=[]\n",
        "kge_scores_diff_lstm=[]\n",
        "\n",
        "\n",
        "for jj in range(0,10):\n",
        "  list_=list(pd.read_csv(\"/content/drive/MyDrive/PHIMP/LSTM_All_50_Weights_RandomNumbers/random_numbers_sim_%d.csv\"%(jj)).iloc[:,1])\n",
        "  model.load_weights(\"/content/drive/MyDrive/PHIMP/LSTM_All_50_Weights_RandomNumbers/Final_sim_%d.h5\"%(jj))\n",
        "\n",
        "  nse_scores_same_lstm.append([])\n",
        "  kge_scores_same_lstm.append([])\n",
        "  nse_scores_diff_lstm.append([])\n",
        "  kge_scores_diff_lstm.append([])\n",
        "  for ii in range(0,420):\n",
        "    temp_x=np.asarray(df_test_tr[df_test_tr['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "       'average_q', 'average_tmax', 'average_tmin']])\n",
        "    temp_y=np.asarray(df_test_tr[df_test_tr['basin_id']==ii]['q']).reshape((-1,1))\n",
        "    xx,yy=split_sequence_multi_train(temp_x,temp_y,365,0,mode='seq')\n",
        "\n",
        "\n",
        "    y_out=yy[:,0].reshape((-1,1))\n",
        "    y_pred=model.predict(xx)\n",
        "    y_m_out=y_pred.reshape((-1,1))\n",
        "\n",
        "    y_out=i_normalize(y_out,mean_q,std_q)[:,0].reshape((-1,1))\n",
        "    y_m_out=i_normalize(y_m_out,mean_q,std_q)\n",
        "\n",
        "    if ii in list_:\n",
        "      nse_scores_same_lstm[jj].append(nash_sutcliffe_error(y_out,y_m_out[:,0].reshape((-1,1))))\n",
        "      kge_scores_same_lstm[jj].append(KGE(y_m_out[:,0].reshape((-1,1)),y_out))\n",
        "    else:\n",
        "      nse_scores_diff_lstm[jj].append(nash_sutcliffe_error(y_out,y_m_out[:,0].reshape((-1,1))))\n",
        "      kge_scores_diff_lstm[jj].append(KGE(y_m_out[:,0].reshape((-1,1)),y_out))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ4McoG0pucx"
      },
      "outputs": [],
      "source": [
        "df_same_lstm = pd.DataFrame(nse_scores_same_lstm)\n",
        "df_same_lstm.to_csv('/content/drive/MyDrive/PHIMP/LSTM_sim_same_nse.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEuhM1KpqKZp"
      },
      "outputs": [],
      "source": [
        "df_same_lstm = pd.DataFrame(kge_scores_same_lstm)\n",
        "df_same_lstm.to_csv('/content/drive/MyDrive/PHIMP/LSTM_sim_same_kge.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BJ-5wSYqZMl"
      },
      "outputs": [],
      "source": [
        "df_diff_lstm = pd.DataFrame(nse_scores_diff_lstm)\n",
        "df_diff_lstm.to_csv('/content/drive/MyDrive/PHIMP/LSTM_sim_diff_nse.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxNU8vRWqng3"
      },
      "outputs": [],
      "source": [
        "df_diff_lstm = pd.DataFrame(kge_scores_diff_lstm)\n",
        "df_diff_lstm.to_csv('/content/drive/MyDrive/PHIMP/LSTM_sim_diff_kge.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02TVgE4jrSXX"
      },
      "outputs": [],
      "source": [
        "import tf_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D6x5dtwqZL5"
      },
      "outputs": [],
      "source": [
        "jj=2\n",
        "ii=399\n",
        "inputs = tf.keras.layers.Input(shape=(365, 9))\n",
        "    #x=tf.keras.layers.Normalization(axis=-1)(inputs)\n",
        "x = tf.keras.layers.LSTM(256, return_sequences=False)(inputs)\n",
        "x=tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(1,activation='linear')(x)\n",
        "    #outputs=tf.keras.layers.Reshape((3,1),name='output')(outputs)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3), loss=\"mse\")\n",
        "model.load_weights(\"/content/drive/MyDrive/PHIMP/LSTM_All_50_Weights_RandomNumbers/Final_sim_%d.h5\"%(jj))\n",
        "nn_without_head = tf.keras.models.Model(inputs=model.inputs, outputs=x)\n",
        "#df_and_nn_model = tfdf.keras.RandomForestModel(preprocessing=nn_without_head,task = tfdf.keras.Task.REGRESSION)\n",
        "df_and_nn_model=tf_keras.models.load_model('/content/drive/MyDrive/PHIMP/Tree models_Sim50/Tree_sim_%d_%d'%(jj,ii))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAlKYWkmQA-i"
      },
      "source": [
        "##Lead time 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7u1-jyHuXw1R",
        "outputId": "83e66090-31d2-4cee-fe64-953471234d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n",
            "36/36 [==============================] - 4s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 12ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 12ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 2s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpnw11km47 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:09.772077. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.485615\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpw8niomtb as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.969731. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.311576\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpzjgifwn8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.803093. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.406843\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp905swezp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.940981. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.195064\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp1fu9qx3_ as temporary training directory\n",
            "Reading training dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7e9f19f7ee60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset read in 0:00:04.882880. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.369996\n",
            "Compiling model...\n",
            "Model compiled.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function InferenceCoreModel.yggdrasil_model_path_tensor at 0x7e9ec9ce3880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp62wcdqse as temporary training directory\n",
            "Reading training dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7e9f19f7ee60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset read in 0:00:04.903411. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.209003\n",
            "Compiling model...\n",
            "Model compiled.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function InferenceCoreModel.yggdrasil_model_path_tensor at 0x7e9ec01280d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpkta6udg6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.909561. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.388377\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpzvovuu_2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.890331. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.301961\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmporwk7abw as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.843366. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.561430\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpm1zvh1mv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.879779. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.277779\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpihk2ggep as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.837057. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.144958\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp60pdo9ho as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.871855. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.134856\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp6g3mp4va as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.807791. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.484293\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpbupgcskv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.950420. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.935602\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpjxrf9lzt as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.839899. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.378579\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpokw6vbcf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.902091. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.865277\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpb6jznskn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.752322. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.231161\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpc674lysv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.877941. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.230244\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpz2xj4q4b as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.935733. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.482510\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp4yoj5rho as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.809252. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.369051\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp2zwl6p06 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.883909. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.656546\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp8zb00kno as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.879874. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.223958\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp_9gqha2_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.818622. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.168745\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp0os6azq8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.952677. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.856079\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp8dhl_nix as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.863951. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.997632\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpkn_bsv1v as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.001144. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.998943\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp1tryz16p as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.896216. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.986379\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp2qvjpvgv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.915830. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.881650\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpgmm7kw_o as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.143098. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.785479\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp6x9_8jd6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.914463. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.696246\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmppvxovr_o as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.986892. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.113599\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp_p_3_lnx as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.851406. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.159402\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpxsjn08cq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.014622. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.605900\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp6531bpgh as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.927497. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.030205\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpj562j7gw as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.040916. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.935314\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpnr1szg49 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.937085. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.229492\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpwao3ha39 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.918444. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.958888\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpezya1q8h as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.885902. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.228090\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpvt5y4v1i as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.814530. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.117923\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp9hesddgs as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.836940. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.334949\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpr8bazt1z as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.157754. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.844985\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpiz7x9csv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.938158. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.273983\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp1uqfqzmn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.978561. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.928619\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpj5x5qosd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.989621. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:11.339666\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpafpu1kvr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.948838. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.889941\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpr_0uyhra as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.823569. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:11.042705\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpj4s8via6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.920257. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.232025\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpbsyu_9qa as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.013623. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.387532\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpd7ocx91s as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.017108. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.816360\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp08vdp0ez as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.899572. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.721018\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpuhqo_l4i as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.085094. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.835157\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmphtz6zw1s as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.951844. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.425442\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpl7m5g24d as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.961402. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.149005\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp0dlau13v as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.901881. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.766047\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp42a8q3cf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.081596. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.246862\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp09mk1brm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.902944. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.491600\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp1n2uvxvf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.018264. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.219942\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpwkis3x9z as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.810630. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.625345\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpw3hjp4vy as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.945947. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.372386\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpgvc2nauo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.954033. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:11.102363\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpc9smufe3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.864858. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:11.103552\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpnlbnpf9e as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.375507. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.919222\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpmzyjuufm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.959350. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.335154\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp_o7fqunq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.065307. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.111239\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpie2gllz5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.891730. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:11.577963\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpr739rkf2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.900245. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.530607\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpzal1no_u as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.816595. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.811153\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpxjpa02oh as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.970593. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.288308\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpinwm9e3i as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.904304. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.805557\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpfqy4qm4o as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:07.303901. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.960891\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpwkd0cj2l as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.023484. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.663906\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpm52ozcmv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.065399. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.709591\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp97ho23ze as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.883229. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.995632\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpj4g3rypq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.019091. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.558118\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpeqg9owi0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.976894. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.672078\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmps7w2lxsl as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.962236. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.641827\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp8vrmsqho as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.830524. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.024885\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmprd67ltn2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.960598. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.977167\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpf544wjdm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.211904. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.708495\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpw6rrct8k as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.024384. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.788509\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpu5loyhm3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.113199. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.292688\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpo7jg9teq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.873793. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.861888\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpsk3v6ync as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.997524. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.964332\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpwh9m53yu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.961597. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.889880\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpmaqxf8nv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.003744. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.187212\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpr4hiihvt as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.830099. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.776712\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmptqqg44up as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.894474. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.717322\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp0gryu31t as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.197751. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.533116\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp05key2un as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.994716. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.314914\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmphxlsy_dx as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.049776. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.334290\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpg6r60ili as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.948562. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.928187\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp6qgxakq1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.876588. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.083514\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpwjzw958u as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.131688. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.510664\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpqg8u_ibq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.866268. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.852469\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp1zz_435n as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.986932. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.601000\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpm9pmeiz0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.878656. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.038626\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpj0xs_okr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.984110. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.525846\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp7dn3czcc as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.258944. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.539065\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpuwukpdoj as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.946468. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.455624\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpsy5kqs7l as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.088129. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.659806\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpo80v4774 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.891657. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.491314\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpiy2j_46b as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.049399. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.066989\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpdi5zt7do as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.867011. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.128765\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp0wvmwf1b as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.057583. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.206965\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp0vsnjryc as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.883563. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.751433\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpn8o1iell as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.930070. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.429442\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpqsnh8ost as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.905253. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.657857\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpesragdue as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.019143. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.265909\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpsdd5femp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.269021. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.404135\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpu4vnkig2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.922592. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.939587\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpu4r9473d as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.102292. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.004567\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpkjw5qa0y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.884220. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.865182\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpknje7bfy as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.004564. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.581466\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp06ybqr15 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.978180. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.953393\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp3j_t7bew as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.012897. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.551321\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpkf_126d8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.903608. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.012208\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpayyu_029 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.036366. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.451798\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmphhuwrmci as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.855962. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.154161\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpz3i2cs8_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.923843. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.530322\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmph_3jkkww as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.364785. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.450866\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp547xg312 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.984750. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.858590\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpp4xwg5nk as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.185734. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.446336\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp88urtcum as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.039833. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.184722\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp288wttoj as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.137338. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.477691\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp_2y_azi7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.954548. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.104952\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpcd6ac1z3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.115191. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.518267\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpzlsjwwr8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.952280. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.129749\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpea8dnr93 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.059627. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.493148\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp95qn40s1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.913882. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.036216\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpql9i1b76 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.025897. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.507072\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmptmdzxesu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.919358. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.974944\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpm5sqvush as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.227342. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.238250\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmptc8hr_cf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.211293. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.909288\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp0205d3pk as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.090835. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.170815\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp981ytabb as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.083455. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.072994\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpo3cvaf33 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.923200. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.122276\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpcaupqmnn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.044563. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.932682\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpx0fyz9hp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.935040. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.963733\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmprxvckc0r as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.952295. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.905219\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 15ms/step\n",
            "Use /tmp/tmplxw5ixme as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.950909. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.780116\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpjs2e09cz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.864476. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.976209\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpc8l8iydd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.907577. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.583737\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmprr8qogli as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.959201. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.124571\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp097a5l0a as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.936994. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.609375\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp3h765tft as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.297955. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.266116\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpcmtojyxd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.058076. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.765204\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpuqgjw72t as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.077509. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.338777\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp6n1wud08 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.954389. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.976576\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp5pjvuyk1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.236869. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.398384\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp19bba_rr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.845492. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.902590\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpheov0txb as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.101931. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.372568\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpfyibo10q as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.926312. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.689778\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpigiif69d as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.063881. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.328245\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpmig764he as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.914026. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.723317\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpr8ztmub7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.001634. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.497585\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp1mmzkjz8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.892862. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.784935\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpu1feu6sv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.901060. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.621631\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp37ireq76 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.905539. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.432040\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpj5xqwkki as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.206189. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.741080\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpx1dl8erz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.010320. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.174800\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpm9to3o04 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.018741. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.741744\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp8xpmvflt as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.973433. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.930963\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpv1ntybmo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.993570. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.251289\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp5wdh094r as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.014596. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.897134\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpwvmtu0ov as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.033552. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.445334\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpvheushua as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.849612. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.839520\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpeccjdmy3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.032176. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.526027\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpahdmii38 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.810894. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.849235\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmp6l07ypza as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.055808. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.255147\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp0k_sdlgg as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.813767. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.864102\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpjgvctu70 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.108550. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.234124\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmpaygwm8np as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:04.809022. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.739449\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpw73cpjut as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:09.671194. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:10.187294\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpnnel2gbs as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.212268. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.760809\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n",
            "Use /tmp/tmpqp13hpwu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.077885. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.926214\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 14ms/step\n",
            "Use /tmp/tmp3fjr5_f2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.063916. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:09.723219\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 13ms/step\n"
          ]
        }
      ],
      "source": [
        "df_tr = df_train.iloc[:,1:].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
        "\n",
        "\n",
        "\n",
        "for jj in range(9,10):\n",
        "  print(jj)\n",
        "  list_=list(pd.read_csv(\"/content/drive/MyDrive/PHIMP/LSTM_All_50_Weights_RandomNumbers/random_numbers_sim_%d.csv\"%(jj)).iloc[:,1])\n",
        "\n",
        "\n",
        "  nse_scores_same_lstm=[]\n",
        "  kge_scores_same_lstm=[]\n",
        "\n",
        "  nse_scores_diff_lstm=[]\n",
        "  kge_scores_diff_lstm=[]\n",
        "  inputs = tf.keras.layers.Input(shape=(365, 9))\n",
        "    #x=tf.keras.layers.Normalization(axis=-1)(inputs)\n",
        "  x = tf.keras.layers.LSTM(256, return_sequences=False)(inputs)\n",
        "  x=tf.keras.layers.Dropout(0.2)(x)\n",
        "  outputs = tf.keras.layers.Dense(1,activation='linear')(x)\n",
        "    #outputs=tf.keras.layers.Reshape((3,1),name='output')(outputs)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3), loss=\"mse\")\n",
        "  model.load_weights(\"/content/drive/MyDrive/PHIMP/LSTM_All_50_Weights_RandomNumbers/Final_sim_%d.h5\"%(jj))\n",
        "  nn_without_head = tf.keras.models.Model(inputs=model.inputs, outputs=x)\n",
        "  #df_and_nn_model = tfdf.keras.RandomForestModel(preprocessing=nn_without_head,task = tfdf.keras.Task.REGRESSION)\n",
        "\n",
        "  for ii in range(0,420):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      temp_x=np.asarray(df_tr[df_train['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "          'average_q', 'average_tmax', 'average_tmin']])\n",
        "      temp_y=np.asarray(df_tr[df_train['basin_id']==ii]['q']).reshape((-1,1))\n",
        "      xx,yy=split_sequence_multi_train(temp_x,temp_y,365,0,mode='seq')\n",
        "\n",
        "      if (jj==9) and (ii<244):\n",
        "        df_and_nn_model=tf_keras.models.load_model('/content/drive/MyDrive/PHIMP/Tree models_Sim50/Tree_sim_%d_%d'%(jj,ii))\n",
        "      else:\n",
        "        df_and_nn_model = tfdf.keras.RandomForestModel(preprocessing=nn_without_head,task = tfdf.keras.Task.REGRESSION,num_trees=100)\n",
        "        df_and_nn_model.fit(x=xx,y=yy[:,0].reshape((-1,1)))\n",
        "        df_and_nn_model.save('/content/drive/MyDrive/PHIMP/Tree models_Sim50/Tree_sim_%d_%d'%(jj,ii))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      temp_xx=np.asarray(df_test_tr[df_test_tr['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "          'average_q', 'average_tmax', 'average_tmin']])\n",
        "      temp_yy=np.asarray(df_test_tr[df_test_tr['basin_id']==ii]['q']).reshape((-1,1))\n",
        "      xx_,yy_=split_sequence_multi_train(temp_xx,temp_yy,365,0,mode='seq')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      y_out_tr=yy_[:,0].reshape((-1,1))\n",
        "      y_pred_tr=df_and_nn_model.predict(xx_)\n",
        "      y_m_out_tr=y_pred_tr.reshape((-1,1))\n",
        "\n",
        "      y_out_tr=y_out_tr*std_q+mean_q\n",
        "      y_m_out_tr=y_m_out_tr*std_q+mean_q\n",
        "\n",
        "      if ii in list_:\n",
        "        nse_scores_same_lstm.append(nash_sutcliffe_error(y_out_tr,y_m_out_tr[:,0].reshape((-1,1))))\n",
        "        kge_scores_same_lstm.append(KGE(y_m_out_tr[:,0].reshape((-1,1)),y_out_tr))\n",
        "      else:\n",
        "        nse_scores_diff_lstm.append(nash_sutcliffe_error(y_out_tr,y_m_out_tr[:,0].reshape((-1,1))))\n",
        "        kge_scores_diff_lstm.append(KGE(y_m_out_tr[:,0].reshape((-1,1)),y_out_tr))\n",
        "\n",
        "  df_same_lstm = pd.DataFrame(nse_scores_same_lstm)\n",
        "  df_same_lstm.to_csv('/content/drive/MyDrive/PHIMP/Tree_sim_same_nse_%d.csv'%(jj), index=False)\n",
        "\n",
        "  df_same_lstm = pd.DataFrame(kge_scores_same_lstm)\n",
        "  df_same_lstm.to_csv('/content/drive/MyDrive/PHIMP/Tree_sim_same_kge_%d.csv'%(jj), index=False)\n",
        "\n",
        "  df_diff_lstm = pd.DataFrame(nse_scores_diff_lstm)\n",
        "  df_diff_lstm.to_csv('/content/drive/MyDrive/PHIMP/Tree_sim_diff_nse_%d.csv'%(jj), index=False)\n",
        "\n",
        "  df_diff_lstm = pd.DataFrame(kge_scores_diff_lstm)\n",
        "  df_diff_lstm.to_csv('/content/drive/MyDrive/PHIMP/Tree_sim_diff_kge_%d.csv'%(jj), index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQM5FLWSxIEI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBprpdt2vnGr",
        "outputId": "0a0dd077-c095-4476-b078-62ecffc8ebad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "143"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ii"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL7vMRTvN9lk",
        "outputId": "c332f3f1-7d2d-4867-a02e-b90f9dad2587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        }
      ],
      "source": [
        "nse_scores=np.zeros((len(list_),1))\n",
        "for ii in list_:\n",
        "\n",
        "  temp_x=np.asarray(df_tr[df_train['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "       'average_q', 'average_tmax', 'average_tmin']])\n",
        "  temp_y=np.asarray(df_tr[df_train['basin_id']==ii]['q']).reshape((-1,1))\n",
        "  xx,yy=split_sequence_multi_train(temp_x,temp_y,365,0,mode='seq')\n",
        "\n",
        "\n",
        "  y_out=yy[:,0].reshape((-1,1))\n",
        "  y_pred=model.predict(xx)\n",
        "  y_m_out=y_pred.reshape((-1,1))\n",
        "\n",
        "  y_out=i_normalize(y_out,mean_q,std_q)[:,0].reshape((-1,1))\n",
        "  y_m_out=i_normalize(y_m_out,mean_q,std_q)\n",
        "\n",
        "  df_y_out=pd.DataFrame(y_out)\n",
        "  df_y_m_out=pd.DataFrame(y_m_out)\n",
        "  df_y_out.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/Obs3_%dtrain.csv'%(ii), index=False)\n",
        "  df_y_m_out.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/Sim3_%dtrain.csv'%(ii), index=False)\n",
        "\n",
        "\n",
        "  nse_scores[list_.index(ii),0]=nash_sutcliffe_error(y_out,y_m_out[:,0].reshape((-1,1)))\n",
        "\n",
        "nse_scores_df = pd.DataFrame(nse_scores)\n",
        "nse_scores_df.to_csv('/content/drive/MyDrive/PHIMP/LSTM50_sim3_nse_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeAq2fRiPYlJ",
        "outputId": "bed76b04-5450-491a-9098-b1b639e78ef3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.70188301])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train\n",
        "np.median(nse_scores,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTFjYJ_IdytZ",
        "outputId": "a49b15f4-283d-45bf-9738-8a14aabb6415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.68602474])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(nse_scores_tr,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW0rbqziDaz0",
        "outputId": "84aebaae-329b-41bf-c98b-e9890fd2084c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7028876679479361"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(kge_scores_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFhEnDZGXml9",
        "outputId": "11caed76-067f-4b5f-e893-6791b77e1a40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.58822238])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(nse_scores_tr,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaIVEc4xdeUq"
      },
      "outputs": [],
      "source": [
        "list_all=[ii for ii in range(0,421)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF7R4ACcdpUX"
      },
      "outputs": [],
      "source": [
        "# prompt: remove similar elements between random numbers and list_all, and save it in a new list\n",
        "list_diff = list(set(list_all) - set(list_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mbcupJY_sAek",
        "outputId": "93f9ca97-a2f4-4d1e-e219-7bfeb1c68558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.55109909])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "LT=0\n",
        "nse_scores=np.zeros((len(list_diff),1))\n",
        "kge_scores_tr=np.zeros((len(list_diff),1))\n",
        "for ii in list_diff:\n",
        "  temp_x=np.asarray(df_test_tr[df_test_tr['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "       'average_q', 'average_tmax', 'average_tmin']])\n",
        "  temp_y=np.asarray(df_test_tr[df_test_tr['basin_id']==ii]['q']).reshape((-1,1))\n",
        "  xx,yy=split_sequence_multi_train(temp_x,temp_y,365,0,mode='seq')\n",
        "\n",
        "\n",
        "  y_out=yy[:,0].reshape((-1,1))\n",
        "  y_pred=model.predict(xx)\n",
        "  y_m_out=y_pred.reshape((-1,1))\n",
        "\n",
        "  y_out=i_normalize(y_out,mean_q,std_q)[:,0].reshape((-1,1))\n",
        "  y_m_out=i_normalize(y_m_out,mean_q,std_q)\n",
        "\n",
        "  df_y_out=pd.DataFrame(y_out)\n",
        "  df_y_m_out=pd.DataFrame(y_m_out)\n",
        "  df_y_out.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/LSTMObs3All%d.csv'%(ii), index=False)\n",
        "  df_y_m_out.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/LSTMSim3All%d.csv'%(ii), index=False)\n",
        "\n",
        "\n",
        "  nse_scores[list_diff.index(ii),0]=nash_sutcliffe_error(y_out,y_m_out[:,0].reshape((-1,1)))\n",
        "  kge_scores_tr[list_diff.index(ii),0]=KGE(y_m_out[:,0].reshape((-1,1)),y_out)\n",
        "\n",
        "nse_scores_df = pd.DataFrame(nse_scores)\n",
        "nse_scores_df.to_csv('/content/drive/MyDrive/PHIMP/LSTMall_sim3_nse.csv', index=False)\n",
        "kge_scores_tr_df = pd.DataFrame(kge_scores_tr,columns=['KGE'])\n",
        "kge_scores_tr_df.to_csv('/content/drive/MyDrive/PHIMP/LSTMall_sim3_kge.csv', index=False)\n",
        "\n",
        "np.median(nse_scores,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXZ3jarXs-LR",
        "outputId": "d174524a-29ce-4b1c-f8d7-33df4dac6086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.58403617])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(kge_scores_tr,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBEB1WuGhq75",
        "outputId": "8d7a5da3-7dd3-436b-c0ef-2771aa394c3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-34.8720524])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(nse_scores,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gC-hjPT3tipS",
        "outputId": "19c16bff-7d92-4a25-9b3e-6c7c77653bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use /tmp/tmprn204v3g as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.430113. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.145151\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpw2khek_y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304589. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.165957\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpyxz3wapm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.262788. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.316195\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpjwtzpdw_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.263467. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.447657\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm8a4p64i as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.285065. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.488537\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn32_ucl5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.239826. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.485748\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 10ms/step\n",
            "Use /tmp/tmp8t_arxus as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.365265. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.526871\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp4xpy6mho as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334902. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.482147\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpar77frdt as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304500. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.538758\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpwo3ggg2f as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.239186. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.379097\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0zomw0u9 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.261692. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.531415\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpw8ipmt_1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.278075. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.448540\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpmxd8qscn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.259420. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.104809\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpj845scxp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.261094. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.402227\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpf3j98zj_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.250800. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.535577\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp243z38ti as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.258628. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.440531\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpg816d5x3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.330819. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.535388\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmplv2kr8vh as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.277990. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.456735\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpda5o7mb1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.433757. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.531503\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpw9v6k5n9 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.301301. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.387353\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8mx1nwr7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.281695. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.497608\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpj6azb9nm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.338986. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.440991\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpgep385mu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.269995. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.346374\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp7bsnlci4 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.298606. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.542714\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0pfeqkcy as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.302692. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.459790\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp61sg02pq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.344098. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.496972\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpl9x6gn2b as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.323189. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.448519\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpq09t8o_l as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.347062. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.406456\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpbsoae95g as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.332977. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.672983\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0hz3k0uy as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.352820. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.403198\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8d5i5lat as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.268386. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.394843\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpw_5w2sl6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:06.960841. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.474661\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp58dmphlu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.382606. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.381798\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn9tqn7m0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.359751. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.368795\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpo8mftkpr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.336752. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.358880\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmph7i0wgih as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.269541. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.383434\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2kgrg_s_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.328547. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.451588\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpyyhoeib5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.357666. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.384414\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpjn9138a3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.329247. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.387533\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpww_8bkk_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.422635. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.190743\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn_v37q4j as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.262259. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.232524\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmps98hgq4h as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.268351. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.350904\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpdpxto2u0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.250353. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.469841\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpd7n6l5cq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.241321. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.435882\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3n_g3rix as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.285823. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.782765\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpwcph4bxo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.294324. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.509954\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2vlfbyqv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:07.081256. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.521394\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzh_4o8gg as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.416845. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.446282\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp4jahbhbe as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.359985. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.255205\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpvjg7o344 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.412496. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.247204\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmptxxb978t as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.309249. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.209941\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpou2ixyhk as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.296530. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.409160\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpe2vabvns as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.270539. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.528033\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8jyjthbn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.306456. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.540781\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1_qq0fu0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.275741. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.525687\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpjbyomjyz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334573. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.579861\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp59b_7tq9 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.327024. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.457239\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxe9ssf5y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.298330. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.535185\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpcrxqepl5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.307718. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.570130\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpi70cimqd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.292987. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.477723\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpj62lrn_8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.357651. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.590112\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2nikw6qy as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.310759. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.573787\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpr96gp3bv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.336056. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.578194\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpkykov2fm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.314566. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.578153\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnrxoc6db as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.585793. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.608696\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpypp6fi7g as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.470447. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.337254\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpr5ohz656 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.386531. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.308105\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpmlc0j0er as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.306323. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.485991\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpul61xx2a as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.303607. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.578271\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9wuxhmoa as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.245649. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.471035\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmps070bytv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.254447. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.484091\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpff_mmk24 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.333581. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.502165\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpcwuvlfty as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.337114. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.538887\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpc5z8u3pa as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.298086. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.579534\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpdcwlre0c as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334254. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.535662\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpdvca6d2e as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304337. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.601781\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpqu213a44 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.309650. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.589798\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpwmgqm24a as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.290313. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.603806\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmptx_s00ro as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.288998. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.560189\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpbxk8t9wm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.262636. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.660592\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1n5rqy71 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.282770. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.514155\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp60aobrw1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.308844. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.516770\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpfz51li76 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.330741. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.527074\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpeq91hot2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:07.801736. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.384907\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpbkocrbw6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.533457. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.239757\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpgl3pkf0i as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.444069. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.210096\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp4ev8k73l as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.352719. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.286343\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp7f3w3n2j as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.276048. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.385252\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn2vn8ygt as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.281004. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.540030\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp78x4qasj as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.220427. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.486369\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpvv44wq50 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.270357. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.326463\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpz0f14k92 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.276172. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.716545\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp16w538ty as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.272834. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.550182\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0eclfg_k as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.314688. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.305273\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpvh6z8mov as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.280879. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.357849\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpoo4txrik as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.316725. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.345953\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnlahnd7y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.306690. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.535332\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmptkzhmr14 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.273117. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.577258\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp16re17i0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.246061. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.635910\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpmk8hxz2m as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.300731. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.390059\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8hzglsw_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.282935. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.530876\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9y70m6s7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.256310. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.511439\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpauof17uz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.274725. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.493696\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9ww9eqp6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.282180. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.492994\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpsucyipl1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.306235. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.512980\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpz9u1b4ko as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.604321. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.320678\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpi4gvdo9v as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.529845. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.236305\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpx0wqum16 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.335971. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.195556\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpg31fvizi as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.321180. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.425122\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxueuo69b as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.306629. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.509444\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpjac2yp9s as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.303702. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.398910\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpx2ea777m as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.264813. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.411492\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5h8noono as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.305079. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.508386\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpkivphgz0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.293745. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.446876\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmprssn8w46 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.292921. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.380044\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpqunlws2l as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.293221. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.400930\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpdi0dwsxl as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.273757. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.346607\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8bj4tumb as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.269073. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.473618\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmphihruadp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.292260. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.389617\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmphwc30_k3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.265217. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.433360\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpe0qdr6ct as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.257641. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.453375\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpykelfk4y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.250036. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.422137\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp7iolbgy0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.276139. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.675485\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmph21i4h5p as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.319666. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.530292\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2nrwbhi_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.314062. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.466600\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmps22zunjn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.294919. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.477526\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0ox5f2eu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.316156. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.494496\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpbbjzsd3m as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.303109. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.493556\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpf4kq8d4j as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.344417. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.405762\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpq6jge24i as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.319812. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.509670\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpilfmiu4c as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.617776. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.327976\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9rdfou17 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.502343. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.177596\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm5k7wwa2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.436440. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.298881\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxat3emnd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.323967. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.364378\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxfcx4j21 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.337715. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.470859\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpd6p_csym as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.302450. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.560096\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpi6n2t8iq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.300909. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.495406\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpwxu_o8jn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.285971. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.546168\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpgi88napf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.382085. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.512613\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpotqu2p2s as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.310202. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.606047\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpni761khi as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.291479. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.501944\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp27r57bvw as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.300937. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.601530\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpfskec2i5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.317299. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.436287\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6o4p884t as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.254876. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.605837\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzxx5ykil as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.289779. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.427511\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp07v8kn90 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.330847. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.567604\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 10ms/step\n",
            "Use /tmp/tmp5pxawc84 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.291405. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.404003\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmphiumqchc as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.302923. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.523491\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpw2ultlq0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.294521. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.410366\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9rgc7bqc as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.270890. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.598057\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp03pz6hah as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.261939. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.419062\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpflj29997 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.275992. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.517200\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpkd1obgct as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.287277. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.443003\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpjfy8h_i2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.269524. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.412937\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpncs4uf80 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.280825. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.284929\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp33t2h8ze as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334725. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.442987\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn02feu2q as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.366032. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.475632\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpevzqiqto as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.347783. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.408803\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpv8w7_h47 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.624638. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.015931\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmproxmdhd9 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.489302. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:06.982513\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzdfntau2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.384060. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.077757\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3us4vlv4 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.365794. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.335539\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn26qyueo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.311455. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.242914\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp59qidexr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.315589. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.398759\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpscys4ctp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.331840. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.413329\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpwnoczysm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.290661. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.447830\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpt6vcdsao as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334556. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.468342\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpglorldcu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.396122. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.403557\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn3sj7b1l as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.345896. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.514715\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpmtwxaojk as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.335067. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.483175\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpoypnfbiq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.351416. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.413702\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmplq9w75o1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.357116. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.734665\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp_uo30679 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.340024. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.723968\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpa8x37x6t as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.362865. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.728609\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9sjcqcly as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.283548. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.527881\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpz1hsw030 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.323340. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.545511\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmptcszrexi as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.278133. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.609466\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5a1jyxtr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.351437. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.496564\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6kr_zn5j as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.303296. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.593505\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpskgbol4c as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.266211. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.503548\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp83u8gzra as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.333971. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.554699\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp93u44tgo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334274. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.486175\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnbwtw9y7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.302218. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.445672\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6q03cqor as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.261528. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.528617\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmppbacjfes as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.309884. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.543315\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn7vx7vdn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.308386. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.707639\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpo9by76c0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.326520. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.717293\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmphvac6gn_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.292268. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.370761\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpinibjfs1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.293759. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.424528\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpt1n47kr_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.318057. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.543651\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpu8yxcx5y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.687958. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.444580\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpvq7n0py9 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.475593. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.509527\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzqyoge60 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.429633. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.247930\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3eyut0gq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.428403. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.342154\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5cl_0orv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.375749. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.464102\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmph5t9xgk_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.347731. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.184927\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpykci00ps as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.340780. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.031259\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpcvyi5_51 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.338400. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.050194\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp4w13_a82 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.276995. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.245765\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpd8zz5n7b as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.317506. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.795301\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpujzcsvjf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.347224. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.867464\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp70fp01h7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.309959. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.600266\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnatnspx2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.341997. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.681094\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmppe2cf6w4 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.250387. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.532690\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp_zitlgby as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.363023. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.736068\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpt7skq_py as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.331581. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.944362\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpcy0qzx3j as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.320156. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.798975\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0cc43xw4 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.319645. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.645641\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmprjr2y_ba as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.296980. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.479536\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2m2nc2wi as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.320766. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.341877\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnr938cdp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304686. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.470400\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpa7_h3t4u as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.282061. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.910197\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpspqw2gdv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.285055. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.746517\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6w5oa327 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.302696. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.972600\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpb08d17dp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.315985. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.816573\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1ctxi8w8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.317980. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.757706\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpdez2tksf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.258345. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.711866\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpa6vod9bf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.295329. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.649265\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6fvtj4jq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.296314. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.597666\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpsxqg2yvw as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.290329. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.569833\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpyrz1znn1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.266477. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.596718\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnh_pxvb5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.275022. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.600418\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzaqcshx_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.290739. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.467570\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp78uiugun as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.404494. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.566001\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpcd2w8k_d as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.286708. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.644845\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpojxgtbh3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.346598. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.623017\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmptrt9hres as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.687923. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.319787\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpo9l7qzgu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.545021. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.277797\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpfbr0nwyj as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.478846. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.532668\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpfe_nksdc as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.405312. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.538133\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmplu56qws4 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.413670. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.610529\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5zilihpe as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.383428. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.486813\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp7shinjel as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.324933. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.535051\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5ollq2zg as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.314655. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.756764\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmppohrvpvh as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334447. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.562194\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpf336e269 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.336902. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.706372\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmplpjzmthj as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.337775. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.572257\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpflpybfn_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.293484. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.430225\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpplogmx8y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.306198. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.473015\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzk5733bv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.311118. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.596613\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2dgpozmf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.329578. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.435613\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9og7wb3r as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.311470. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.977436\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 8ms/step\n",
            "Use /tmp/tmp0pncv6bi as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.299930. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.011339\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm95sevae as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.311732. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.576227\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1454h1i1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.332294. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.508315\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2cn8sswo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.350680. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.288869\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5rqltsbq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.312848. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.619044\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp4vkgkdwd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.291110. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.421701\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp_yyv3hi2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.293120. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.544691\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpvqwza4xr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.328898. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.580660\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzek3gzp5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.273048. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.572235\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp69ub6j1z as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.321378. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.674929\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6w5171t_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.314481. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.462473\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpd4l9u83c as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304735. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.519519\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpa4owkl6b as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.384151. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.455243\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp7na3_g_u as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.305110. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.214134\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpmvwg3wos as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.321797. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.998810\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpq3uchi5d as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.335521. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.145956\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmptr6boi5g as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.299676. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.748017\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpy72zjsls as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.309492. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.928584\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0gctphyp as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.299463. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.646222\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpd61c16a9 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.290168. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.658531\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmphh9cbg3g as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.336460. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.915052\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpemm175kl as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.363891. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.648512\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpdg51oobo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.371444. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.763001\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpx0ls93zl as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.349045. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.793654\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpwbi6r_pr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:10.796349. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.559452\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpv3cts1mn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.588908. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.911300\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6yjz86pf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.518258. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.757761\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9yvqpseu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.546426. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.771584\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn72hqswg as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.460969. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.828311\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpna6blo21 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.457196. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.488203\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3eh7j35n as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.442521. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.423212\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 10ms/step\n",
            "Use /tmp/tmpjd4jx6gm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.360680. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.336568\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5igz2g24 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.371118. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.334137\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpufbbltyy as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.350903. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.305596\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpkcyzemuh as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.346763. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.521740\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm0v4r120 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.317465. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.245617\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpot9b4pr_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.334769. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.428684\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpb2r5g323 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.328026. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.516752\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpsyi2l6_2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304788. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.280224\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpx_u6oi3z as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.341035. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.991339\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpykryscv8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.311943. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.480013\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmppu768yuw as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.349435. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.507758\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp22n8aw_r as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.362078. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.946805\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3v7yr_ta as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.418638. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.253844\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmptqobq9e5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.354813. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.291585\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpsm50xfjz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.326414. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.302982\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpcocrs430 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.446012. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.619498\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpego9rdz3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.353017. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.961223\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpd9uol6w7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.297178. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.733033\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3ld15gr1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.324510. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.925102\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0rbn7sxd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.308200. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.024320\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp7w6catpi as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.294851. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:08.042318\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp670frwju as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.316040. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.620089\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxl9tmk6d as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.322003. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.278686\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8g08ggo1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.381969. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.382418\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8skn33t0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304790. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.579139\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnu4q5ynq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.283212. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.437599\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3qfmg1jj as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.280898. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.974982\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp5zgbx7nd as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.374838. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.617514\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpc_h_6vhy as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.291794. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.402688\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpi7g6ybvg as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.345369. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.433177\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpk1d3ksya as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.271141. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.646085\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8hz0c2rs as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.355066. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.461012\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3z0t9w3o as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.335892. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.652251\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpaucc9cnm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.299824. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.663852\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1b7ff2s6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.330184. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.745522\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxf42m_xv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.371686. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.835679\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp_k2knm15 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.363834. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.184406\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm3i93ri5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.353708. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.796229\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm8qdtqsm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.379883. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.168418\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6v0tq6jg as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:11.198126. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.018865\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpf0fvwnz1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.578031. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.876808\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpgcauayzs as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.619386. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.715775\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1ypnkdma as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.542090. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.398288\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpz69rgkj6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.505116. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.440064\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpr9h80ghq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.445316. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.457819\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpvcygvqqz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.434256. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.480077\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp7yx6nsa6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.503049. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.341523\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpo5m2kirr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.381800. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.366019\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxgzl_ywe as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.386444. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.359817\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpe2ph8bbx as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.353469. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.322220\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8dqlbksb as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.363012. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.349163\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0yyd5ny8 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.303974. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.318519\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpt2qgn9sq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.346342. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.228735\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp872jm8a6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.391981. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.308932\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpgx9lo359 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.340367. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.365168\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmprgc8lohr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.335464. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.457312\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpyd_b7458 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.285563. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.375191\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpglxyyif5 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.359328. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.475559\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpy71o6t92 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.359063. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.360329\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm8pcc4mn as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.344419. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.521728\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpxz6ezpt3 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.316783. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.540835\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpm6rwqhxq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.310874. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.379198\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpida40qd_ as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.306393. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.397225\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpp_gckue2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.340045. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.427029\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpiesj2niv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.328544. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.451156\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpnt35__vz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.277186. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.427190\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpkjqu2oho as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.315375. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.385820\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpswmlxzmm as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.316181. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.398840\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpfqzn7nxo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.308368. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.471992\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp_uqxckxq as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.324911. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.391186\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpn391zz2d as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.359423. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.486263\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpl9obirln as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.333601. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.401720\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpypk60jww as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.325999. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.309293\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpbdv3y2ue as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.312309. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.304573\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp2r9cii2x as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.297484. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.237777\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp3p406dm2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.315550. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.232522\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpezbl0d1z as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.303044. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.341287\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpp3tu2k0g as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.319664. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.382188\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmprv9g9vr2 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.331615. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.341828\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp8emg6_nv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.324377. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.289706\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp4x_inb_c as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.304438. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.276225\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp9akrbs2o as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.316937. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.253945\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpwvln4pd4 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.300529. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.332468\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpt9olwasf as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.307599. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.399891\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp225oj5zz as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.337315. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.390791\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpuxyvyoid as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.392525. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.498236\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpd009tvx1 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.367690. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.367352\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpiph7qr1y as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.336804. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.368521\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1n1wqqe4 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.341180. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.115479\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpenccdf8i as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.354751. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.336433\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp6o6crfsi as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.343621. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.392772\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 10ms/step\n",
            "Use /tmp/tmpcy56s7w7 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.803173. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.220871\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 10ms/step\n",
            "Use /tmp/tmporp__7za as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.664051. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.360112\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpdwcmr0c6 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.589181. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.413773\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp4xusw42c as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.494830. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.384919\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpzsoctu41 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.489570. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.500313\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp_woqrmrr as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.462308. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.435215\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp0fdmaf50 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.432376. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.439363\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmp1i_7kf31 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.392087. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.665264\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpskwgw8hv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.403891. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.395944\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpp2vnvbnu as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.381821. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.414764\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n",
            "Use /tmp/tmpvrwe_nfv as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:05.332374. Found 8208 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:07.436975\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "36/36 [==============================] - 1s 9ms/step\n"
          ]
        }
      ],
      "source": [
        "df_tr = df_train.iloc[:,1:].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
        "nse_scores_tr=np.zeros((len(list_diff),1))\n",
        "kge_scores_tr=np.zeros((len(list_diff),1))\n",
        "step=1\n",
        "for ii in list_diff:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  temp_x=np.asarray(df_tr[df_train['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "       'average_q', 'average_tmax', 'average_tmin']])\n",
        "  temp_y=np.asarray(df_tr[df_train['basin_id']==ii]['q']).reshape((-1,1))\n",
        "  xx,yy=split_sequence_multi_train(temp_x,temp_y,365,0,mode='seq')\n",
        "\n",
        "  df_and_nn_model = tfdf.keras.RandomForestModel(preprocessing=nn_without_head,task = tfdf.keras.Task.REGRESSION,num_trees=100)\n",
        "  df_and_nn_model.fit(x=xx,y=yy[:,0].reshape((-1,1)))\n",
        "\n",
        "  temp_xx=np.asarray(df_test_tr[df_test_tr['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "       'average_q', 'average_tmax', 'average_tmin']])\n",
        "  temp_yy=np.asarray(df_test_tr[df_test_tr['basin_id']==ii]['q']).reshape((-1,1))\n",
        "  xx_,yy_=split_sequence_multi_train(temp_xx,temp_yy,365,0,mode='seq')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  y_out_tr=yy_[:,0].reshape((-1,1))\n",
        "  y_pred_tr=df_and_nn_model.predict(xx_)\n",
        "  y_m_out_tr=y_pred_tr.reshape((-1,1))\n",
        "\n",
        "  y_out_tr=y_out_tr*std_q+mean_q\n",
        "  y_m_out_tr=y_m_out_tr*std_q+mean_q\n",
        "\n",
        "  df_y_out_tr=pd.DataFrame(y_out_tr)\n",
        "  df_y_m_out_tr=pd.DataFrame(y_m_out_tr)\n",
        "  df_y_out_tr.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/ObsTrAll%d_sim3.csv'%(ii), index=False)\n",
        "  df_y_m_out_tr.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/SimAll%d_sim3.csv'%(ii), index=False)\n",
        "\n",
        "\n",
        "\n",
        "  nse_scores_tr[list_diff.index(ii)]=nash_sutcliffe_error(y_out_tr,y_m_out_tr)\n",
        "  kge_scores_tr[list_diff.index(ii)]=KGE(y_m_out_tr,y_out_tr)\n",
        "  #print(nse_scores_tr[list_.index(ii)])\n",
        "\n",
        "\n",
        "nse_scores_tr_df = pd.DataFrame(nse_scores_tr,columns=['NSE'])\n",
        "nse_scores_tr_df.to_csv('/content/drive/MyDrive/PHIMP/tr_nn_all_sim3_nse.csv', index=False)\n",
        "\n",
        "kge_scores_tr_df = pd.DataFrame(kge_scores_tr,columns=['KGE'])\n",
        "kge_scores_tr_df.to_csv('/content/drive/MyDrive/PHIMP/tr_nn_all_sim3_kge.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AtxXr29xeQR",
        "outputId": "03db258e-62d4-4013-8402-600f088941ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.7281261])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(nse_scores_tr,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVjj8ewkFVg3",
        "outputId": "6b25c725-d110-4c41-f199-68f09ce590e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.73302868])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(kge_scores_tr,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N_jycAqIP1t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu8reh85FjEe",
        "outputId": "17b146d9-3ec1-46fa-ece1-37a250191b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - loss: 0.1820\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0993\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0886\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0880\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0718\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.1443\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1756\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0997\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step - loss: 0.1076\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step - loss: 0.0839\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - loss: 0.1461\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1313\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1353\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1213\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1343\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2385\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2053\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - loss: 0.2011\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - loss: 0.1774\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - loss: 0.1638\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 36ms/step - loss: 0.8249\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.6630\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.7120\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.7186\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.5821\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.4384\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.3583\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.3775\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.3282\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.3306\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - loss: 0.1769\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1130\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1058\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0940\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1019\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.1124\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0729\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0595\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0571\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0593\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - loss: 0.2253\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2060\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1719\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1765\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1800\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.0692\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.0471\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0477\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - loss: 0.0448\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0412\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.3361\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2388\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2585\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2510\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2156\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.2080\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1673\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1980\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1993\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1763\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - loss: 0.2452\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2309\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1959\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1911\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2144\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.1005\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step - loss: 0.0970\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step - loss: 0.0944\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.0944\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.1006\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - loss: 0.3092\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.4174\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.4198\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2709\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2406\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.3728\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2430\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2283\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - loss: 0.2555\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - loss: 0.3630\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 60ms/step - loss: 0.1529\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1595\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1410\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1071\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1598\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 61ms/step - loss: 0.0801\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0787\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0751\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0644\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0733\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.5962\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4410\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.4663\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.3572\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.3633\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0805\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0625\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0554\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0658\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0564\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.1315\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0990\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1151\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0833\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0867\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.4850\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2357\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1765\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1591\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1259\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 54ms/step - loss: 0.3498\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0726\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0421\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0364\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0314\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.3456\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1264\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1326\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1144\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0812\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.2116\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.1605\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1098\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1154\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1558\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.3113\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1515\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2645\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1462\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2926\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.0943\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1425\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0896\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1008\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0725\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.0888\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0737\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0862\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0727\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0757\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0635\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0595\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0563\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0494\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0524\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0992\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1022\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0974\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0913\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 71ms/step - loss: 0.0805\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0910\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0666\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0709\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0583\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0785\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1524\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0924\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1351\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1073\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0949\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0875\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0633\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0569\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0687\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0556\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.1326\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1023\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0956\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0966\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0924\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1246\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0941\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1008\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0876\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0832\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.0876\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0569\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0399\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0384\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0419\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.0966\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0328\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0304\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0334\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0280\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.1355\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1311\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1276\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1056\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1110\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1096\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1421\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.2642\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0733\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1484\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.1093\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0676\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1458\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0940\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0710\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.3093\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.2302\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1691\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2360\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2120\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0867\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0474\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0805\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0413\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0706\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.1215\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1756\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1066\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1555\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1616\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1206\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1476\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1743\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1251\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1631\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - loss: 0.0751\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0614\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0568\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0599\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0758\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 53ms/step - loss: 0.0820\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0483\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0645\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0542\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0482\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1798\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1150\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1514\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1149\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.1336\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.1354\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0680\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0482\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0675\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0550\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.2186\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1477\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0905\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0825\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0875\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.4188\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1677\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1222\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1464\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1533\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.1242\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0992\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0792\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0780\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0867\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.1484\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0985\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0874\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1515\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0898\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.0751\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0766\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0685\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0497\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0584\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0977\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0576\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0365\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0540\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1058\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1571\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1534\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1830\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1410\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1317\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.3497\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3897\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.3494\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.3401\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.3793\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.2277\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1425\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1205\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1399\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0982\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1511\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1073\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.2007\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0826\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1129\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1405\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1624\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1261\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1309\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1260\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1474\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1012\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1012\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0788\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0798\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.1256\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0933\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0773\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0726\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0744\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0785\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0612\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0858\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0698\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0719\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0612\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0587\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0633\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0600\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0563\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.2244\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0617\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0937\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1078\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0940\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1450\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1051\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1395\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0803\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0830\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0834\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0964\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0715\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0658\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0691\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0868\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0725\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0655\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0787\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0659\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1877\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1414\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0944\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0912\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1279\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.4521\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2047\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.3047\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1369\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.4656\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.1627\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1586\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1041\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0888\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1001\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1336\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1468\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.2009\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0882\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0654\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - loss: 0.0753\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0682\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0690\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0674\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0551\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1729\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1717\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1244\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1003\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0829\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.1387\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1331\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1639\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1091\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1057\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.0716\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.0609\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0695\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0834\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0612\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0641\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0619\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0537\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0531\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0624\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0690\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0687\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0585\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0592\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0743\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0990\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0568\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0702\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0728\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0612\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.2275\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3818\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3587\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2539\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2690\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.0872\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0901\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0761\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0836\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0783\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1347\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0905\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0914\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1070\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0803\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1012\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0700\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0603\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0666\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0634\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.1160\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0855\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0789\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1166\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0718\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.1742\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1163\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0988\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1160\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0955\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.2033\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1284\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1658\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0803\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0718\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1611\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2049\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1430\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1096\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0897\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0468\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0367\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0294\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0333\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0349\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.0876\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0952\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1319\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0616\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0568\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1268\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1474\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1784\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1111\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1167\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1165\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1130\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1113\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0910\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0967\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - loss: 0.0595\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0445\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0519\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0443\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0411\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.1015\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0938\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0750\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0682\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0742\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.4341\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3272\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2957\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3020\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2503\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.1065\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0527\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0378\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0401\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0336\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.2540\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2629\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1277\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0696\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1097\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 50ms/step - loss: 0.2846\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.2855\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3225\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3102\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2723\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.1557\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1364\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.1493\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0949\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0878\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0945\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1102\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0614\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0540\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0648\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.5689\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.6131\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.6425\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.4045\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3528\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.2014\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1512\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1025\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0908\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0626\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0940\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0844\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0991\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0801\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0656\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.3927\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3322\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1788\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1409\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1608\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.3117\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2774\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2093\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.1879\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1293\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 0.2444\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2213\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2046\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1498\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.1790\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.9628\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.7727\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3388\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2887\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3827\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.4013\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2915\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2237\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1897\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1761\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.2426\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2618\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2116\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1591\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.1397\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.1114\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.1006\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1075\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1011\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0894\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1842\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1646\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1371\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1201\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0955\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 1.0302\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.7956\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.8492\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.8661\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.9100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.2975\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3170\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3258\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.2165\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1996\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.2479\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2038\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1752\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1453\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1485\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 54ms/step - loss: 0.8928\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.7553\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.7245\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.6720\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.7078\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.4050\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3285\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3399\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.3106\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2735\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1035\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0745\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0783\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0697\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0722\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.1238\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0887\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0896\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0895\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0897\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.1301\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.1107\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1122\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1034\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0971\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.4553\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.3862\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3774\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.3393\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3443\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.1016\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - loss: 0.1068\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1037\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0878\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1030\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1183\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0736\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0817\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0631\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0802\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.2535\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1614\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1271\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1405\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1322\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1333\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1372\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1267\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1176\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1266\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - loss: 0.1366\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1320\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1304\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1350\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1206\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1313\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0899\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0925\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0673\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0797\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0443\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0378\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0268\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0251\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0207\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0544\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0407\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0416\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0345\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0427\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1865\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1388\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1624\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1839\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1457\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.1083\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0863\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0888\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0732\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0886\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.3447\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2376\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2251\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2174\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2721\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.2935\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1758\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1448\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1832\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1575\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.3828\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2251\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2566\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2462\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2773\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0679\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0737\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0688\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0609\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0665\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.4273\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3133\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2277\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2285\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2901\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.2378\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1136\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1086\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1129\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0855\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0831\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1752\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0964\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1374\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1052\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.2341\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1833\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2015\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1484\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1925\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1016\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0995\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0973\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0996\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0998\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.2173\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1639\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1547\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1457\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1390\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.2193\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1832\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1577\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1348\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1570\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.2961\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3145\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3763\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2311\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1931\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1017\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0729\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0779\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0648\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0755\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.7107\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.5904\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3146\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.5037\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.4126\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 0.0540\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0610\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0540\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0581\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0573\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.2113\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1746\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2389\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1483\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1332\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0624\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0506\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0458\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0451\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0435\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.2673\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0853\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0832\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0840\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0703\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0809\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0562\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.0554\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0542\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0543\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0867\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0792\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0679\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0629\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0601\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.2541\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1754\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1569\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1489\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1446\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.3070\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1900\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1719\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2262\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1387\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.0918\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0726\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0563\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0544\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0474\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0709\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0614\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0407\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0432\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0355\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0727\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0484\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0383\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0357\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0313\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0504\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0252\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0198\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0170\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0157\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0303\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0167\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0159\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0143\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0143\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0703\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0667\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0562\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0456\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0480\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0284\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0234\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0223\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0205\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0181\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.0191\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0164\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0148\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0135\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0133\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0476\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0169\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0104\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0070\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0065\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0298\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0114\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0124\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0157\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0090\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0272\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0110\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0082\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0069\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0147\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.0268\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0056\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0051\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0041\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0039\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0543\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0462\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.0419\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0388\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0346\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.1240\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - loss: 0.1055\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1147\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1042\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0907\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1140\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0870\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0986\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0855\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0852\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.2385\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - loss: 0.2716\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2465\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2019\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2096\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1430\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.1425\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1155\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1196\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1135\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1270\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0783\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0678\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0541\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0567\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.1376\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0873\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0782\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0746\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0703\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1226\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0997\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0984\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1024\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1051\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - loss: 0.0573\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0550\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0482\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0515\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0527\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0086\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0065\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0073\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0058\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0054\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - loss: 0.0044\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0041\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0033\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0037\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0036\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0237\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0229\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0204\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0188\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0170\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1116\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0765\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0789\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0561\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0547\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1737\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1314\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1279\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1030\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1067\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.2241\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1823\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2162\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1560\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1649\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0114\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0088\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0108\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0094\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0104\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0201\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0229\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0198\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0197\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0326\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0474\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0351\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0426\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0453\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0422\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.1878\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1611\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1277\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0882\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1034\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - loss: 0.1087\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0742\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0766\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0797\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0888\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0987\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0906\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0755\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0879\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0862\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.1575\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1322\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0938\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1236\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1131\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1379\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1599\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1003\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1416\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1075\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - loss: 0.3032\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1822\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2342\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1659\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2175\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.2329\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1782\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1566\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1606\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2125\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1031\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0831\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0851\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0635\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0767\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0862\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0759\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0694\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0673\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0660\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1166\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1478\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1021\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0880\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1036\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.2116\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1841\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2359\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1818\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1566\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.2734\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.4099\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2729\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2645\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.3694\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0665\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0411\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0387\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0381\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0331\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0520\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0280\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0263\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0236\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0240\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0120\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0106\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0088\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0077\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0079\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0059\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0061\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0052\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0061\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0046\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0114\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0117\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0106\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0080\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0135\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0118\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0092\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0095\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0082\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0099\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0042\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0043\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0047\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0042\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0049\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0072\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0049\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0059\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0040\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0041\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0032\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0026\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0024\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0023\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0025\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0039\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0023\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0019\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0020\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0019\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0041\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0029\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0028\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0026\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0024\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0022\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0019\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0019\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0021\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0018\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0254\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0181\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0225\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0183\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0146\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.0076\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0063\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0072\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0051\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0056\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.0264\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0224\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0193\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0241\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0182\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0355\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0230\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0471\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0306\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0215\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0877\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0573\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0491\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0511\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0477\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.0619\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0383\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0321\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - loss: 0.0322\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0277\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.0929\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0554\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0547\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0553\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0465\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0595\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2133\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1120\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1057\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0963\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.2329\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0844\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2026\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0964\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0533\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.1092\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0660\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0650\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0554\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.0631\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0031\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0021\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0018\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0020\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0017\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0137\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0104\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0136\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0086\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0089\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0192\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0207\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0163\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0169\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0179\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0448\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0341\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0349\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0347\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0382\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.0766\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0688\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0715\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0474\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0510\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1447\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1083\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1081\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1218\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 40ms/step - loss: 0.1001\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.3217\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.4267\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1283\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1266\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.2744\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.1721\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1189\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1821\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1294\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1542\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.1758\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.1302\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.1210\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1351\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1314\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.2157\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1215\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1692\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1631\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1228\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.1590\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.1290\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1019\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1081\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1079\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.3170\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2814\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2613\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1872\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2343\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 54ms/step - loss: 0.0697\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0849\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0842\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0707\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0704\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.1742\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1231\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1163\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1194\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0945\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.2917\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 85ms/step - loss: 0.2993\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.3045\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2815\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.2138\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0731\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0523\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0386\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0425\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0337\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 48ms/step - loss: 0.1890\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2237\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1446\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1275\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1239\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 50ms/step - loss: 0.1053\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1129\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1008\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1036\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0895\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0065\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0043\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0032\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0033\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0032\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.6655\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.9088\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4309\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.8080\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4973\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.2125\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1996\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1212\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1721\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.1132\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.7059\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.9421\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.4436\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4798\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.6459\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.6059\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.5824\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.6941\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.5626\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4121\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0071\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0057\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0043\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0038\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0039\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.2481\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2779\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1843\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2243\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2877\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 1.3802\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.4109\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 1.6025\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.0304\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 1.1523\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0037\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0026\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0020\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0019\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0019\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0817\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0596\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0512\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0469\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0761\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.1248\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0756\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0700\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0779\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0713\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 2.0971\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 1.5220\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 1.3941\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 1.4581\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 53ms/step - loss: 1.4302\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 1.4713\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.8052\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 1.0959\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.8463\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 1.0637\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.1520\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0625\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0772\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0587\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0576\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.6702\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.3914\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.4589\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.4134\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.4222\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.3712\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.3885\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.3410\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3208\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2574\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.2143\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.1890\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1301\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1455\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1534\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.7553\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.5821\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.4191\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3931\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.3049\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.8818\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.4360\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.3797\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.5701\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.3512\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.9250\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 1.7849\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.9197\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.7263\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.6250\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.1827\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2342\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1616\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1439\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1101\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.1325\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2415\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1198\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1393\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.1747\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.1797\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1437\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1355\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1043\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1270\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0374\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0385\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0441\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0602\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0331\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.0350\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0210\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0276\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0219\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0141\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0284\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0320\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0273\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0193\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0164\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0810\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0574\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0461\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0655\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0732\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0300\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0386\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0433\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0222\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0185\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - loss: 0.1741\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1284\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0973\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1141\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0714\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.1925\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2191\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1155\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0949\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1008\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.2681\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2419\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1350\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0781\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1705\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.0946\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1438\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1025\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0894\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0756\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.2004\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.6042\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.7655\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3989\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.8757\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0581\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0596\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0543\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0377\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0310\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0429\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0225\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0321\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0314\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0570\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0216\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0149\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0139\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0271\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0188\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0602\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1508\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0584\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0646\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0517\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.2377\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2083\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2575\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0717\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1726\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - loss: 1.6147\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.3820\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 2.4704\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3915\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2193\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.1312\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0735\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2385\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1047\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1498\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 53ms/step - loss: 0.0133\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0072\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0056\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0049\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0047\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - loss: 0.0034\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 52ms/step - loss: 0.0029\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0025\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0023\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0023\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 54ms/step - loss: 0.0336\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0201\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0161\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0185\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0171\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0278\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0115\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0099\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0107\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0092\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0111\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0105\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0079\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0150\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0082\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0339\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0240\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0218\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - loss: 0.0228\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0242\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.0059\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0050\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0054\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0049\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0045\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0108\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0060\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 57ms/step - loss: 0.0049\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0044\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0040\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 50ms/step - loss: 0.1417\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0757\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0781\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0813\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0794\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 0.0092\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0098\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0096\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0093\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0067\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0028\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0020\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0018\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0017\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0016\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 52ms/step - loss: 0.1096\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0749\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1079\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0995\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 52ms/step - loss: 0.1804\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0101\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0059\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0109\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0042\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0100\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0132\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0089\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0085\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0045\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0071\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0435\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0243\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0171\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0154\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0161\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0070\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0052\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0050\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0046\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0039\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0062\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0075\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0065\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0115\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0060\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.0986\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0759\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0614\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0440\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0417\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0363\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0236\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0276\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0169\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0248\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0426\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0722\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0197\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0152\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0169\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0385\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0399\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0251\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0151\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0133\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.1520\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0785\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1256\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0849\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0569\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.0026\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0018\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0019\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0017\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0016\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.0072\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0044\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0037\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0032\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0030\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0152\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0072\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0058\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0063\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0059\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 50ms/step - loss: 0.0434\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0321\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0274\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0179\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0131\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0298\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0188\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0128\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0117\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0120\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.0151\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0158\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0077\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0105\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0073\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 54ms/step - loss: 0.1164\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0972\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0827\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0767\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0778\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.0059\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0047\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0055\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0050\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0060\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0254\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.0166\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0127\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0116\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 0.0153\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0718\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0403\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0325\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0327\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0295\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.3353\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2789\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2375\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2717\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2610\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.5084\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.4190\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1755\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2521\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.3236\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 54ms/step - loss: 0.0052\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0049\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0049\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0037\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0042\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0419\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0409\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0398\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0352\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0381\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0067\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0049\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0041\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0051\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0044\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.1364\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1005\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0881\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0930\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0754\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0040\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0055\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0035\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0033\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0038\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - loss: 0.4275\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 52ms/step - loss: 0.2416\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.1834\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1324\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0966\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.7455\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.3097\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2335\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1613\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1504\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - loss: 0.0326\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0169\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0143\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0128\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0116\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0119\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0112\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0094\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0069\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0093\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 54ms/step - loss: 0.3142\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - loss: 0.1547\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.1254\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1150\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0981\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - loss: 0.2171\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.0929\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0798\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1393\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0571\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.7405\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.9302\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.4244\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.4170\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.5669\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.2610\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2936\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2369\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2040\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2449\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.4084\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.2429\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2154\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.2257\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.2169\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - loss: 1.1710\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.9097\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.8222\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.6951\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.7193\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.7823\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.5423\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.4388\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.4586\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.4179\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.6887\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.5019\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4288\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.4213\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.4747\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.3884\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3160\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.3783\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.3003\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2897\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 49ms/step - loss: 0.4388\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2616\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1950\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1789\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.1565\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 53ms/step - loss: 0.6232\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - loss: 0.3680\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.3119\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3464\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - loss: 0.2533\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - loss: 0.2643\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1451\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1360\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1244\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.1199\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.9371\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.7501\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.6668\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.6707\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.8516\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 1.0669\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.9440\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.8859\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 1.0220\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.7670\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.7034\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.4709\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.3733\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3682\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.4269\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 1.8085\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.2472\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 1.2849\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.9861\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.7283\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.9618\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.6479\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.6017\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4813\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4423\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 57ms/step - loss: 2.0205\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - loss: 1.9169\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 1.5968\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 1.5625\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.6290\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 2.0428\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 1.5483\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 1.5235\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 1.6658\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 1.6132\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 1.2109\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.0031\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.9930\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.9051\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.8582\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 2.3360\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 2.0442\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 1.9708\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 1.9161\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 1.9199\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 61ms/step - loss: 0.7404\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4628\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.4105\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3828\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.3231\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 1.0323\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.6189\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.5518\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.4585\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.4546\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.6540\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.5287\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.4160\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.4405\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.3994\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.8678\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 48ms/step - loss: 0.6984\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.5109\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.5044\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.4993\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.6305\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.4311\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.3744\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3404\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.3481\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 1.7248\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 1.4644\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 1.2303\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.1031\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 1.1916\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 1.4208\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 1.0763\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 1.1083\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 1.0575\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 1.0442\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 1.0924\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.6689\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.4890\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.4461\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.3971\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.5256\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3174\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 78ms/step - loss: 0.2459\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - loss: 0.2693\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2574\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.1317\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0313\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0227\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0216\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0191\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - loss: 0.1662\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0427\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0296\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0316\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0285\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.5455\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.2835\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2091\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.1483\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.1395\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 1.3874\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.7026\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - loss: 0.4293\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.2988\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.2820\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - loss: 1.2714\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.6162\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.4142\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2769\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2222\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.1679\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0550\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0485\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0411\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - loss: 0.0444\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.0455\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0280\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0278\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0225\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0208\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - loss: 0.0028\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0021\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0021\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0018\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.0016\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.0046\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.0028\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0026\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0025\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0024\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.0350\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.0201\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0171\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.0170\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0145\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.5639\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.3074\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2536\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.2116\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2001\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.2105\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.1118\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0712\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.0494\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.0402\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 41ms/step - loss: 0.1414\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0654\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0566\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0594\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0596\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.0452\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.0229\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0160\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0166\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0120\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.5419\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3764\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.3189\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.3721\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.2599\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1086\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1010\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.0990\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1006\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.0966\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 53ms/step - loss: 0.6016\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.2803\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 0.1927\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1551\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.1395\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1930\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1522\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1229\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.1110\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1013\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 1.4916\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 1.6008\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.7369\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 1.8831\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.9374\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.6841\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - loss: 0.3977\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.3542\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2572\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.2795\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 1.6103\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - loss: 0.9735\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.7805\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.7422\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.7345\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 50ms/step - loss: 0.5144\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - loss: 0.5283\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.4118\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3534\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.4264\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - loss: 0.8067\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.5912\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.6447\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.5193\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.5352\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.2504\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.1577\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1484\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.1493\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1324\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.7698\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.7382\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.6283\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.6383\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.6606\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 42ms/step - loss: 0.2741\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2118\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.2282\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.2333\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1780\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - loss: 0.2737\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.3072\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.2513\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.2674\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 0.2524\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 0.1941\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 0.1579\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 0.1741\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1989\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.1639\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 61ms/step - loss: 1.0716\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.8245\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.6675\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - loss: 0.6849\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 0.6021\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step - loss: 2.6124\n",
            "Epoch 2/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 1.8922\n",
            "Epoch 3/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 1.4491\n",
            "Epoch 4/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 1.4439\n",
            "Epoch 5/5\n",
            "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - loss: 1.3903\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n"
          ]
        }
      ],
      "source": [
        "df_tr = df_train.iloc[:,1:].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
        "nse_scores_tr=np.zeros((len(list_diff),1))\n",
        "kge_scores_tr=np.zeros((len(list_diff),1))\n",
        "\n",
        "def modeli():\n",
        "  inputs = tf.keras.layers.Input(shape=(365, 9))\n",
        "  #x=tf.keras.layers.Normalization(axis=-1)(inputs)\n",
        "  x = tf.keras.layers.LSTM(256, return_sequences=False)(inputs)\n",
        "  x=tf.keras.layers.Dropout(0.2)(x)\n",
        "  outputs = tf.keras.layers.Dense(1,activation='linear')(x)\n",
        "  #outputs=tf.keras.layers.Reshape((3,1),name='output')(outputs)\n",
        "  model_ = tf.keras.Model(inputs, outputs)\n",
        "  return model_\n",
        "\n",
        "\n",
        "\n",
        "for ii in list_diff:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  temp_x=np.asarray(df_tr[df_train['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "       'average_q', 'average_tmax', 'average_tmin']])\n",
        "  temp_y=np.asarray(df_tr[df_train['basin_id']==ii]['q']).reshape((-1,1))\n",
        "  xx,yy=split_sequence_multi_train(temp_x,temp_y,365,0,mode='seq')\n",
        "\n",
        "\n",
        "  model_=modeli()\n",
        "  model_.load_weights(\"/content/drive/MyDrive/PHIMP/Checkpoint/Final50_sim.h5\")\n",
        "  model_.compile(optimizer=tf.optimizers.Adam(learning_rate=5e-6), loss=\"mse\")\n",
        "\n",
        "\n",
        "  history = model_.fit(x=xx,y=yy, epochs=5,batch_size=32)\n",
        "  model_.save_weights(\"/content/drive/MyDrive/PHIMP/Checkpoint/simlstmfine_%d.weights.h5\"%(ii))\n",
        "\n",
        "  temp_xx=np.asarray(df_test_tr[df_test_tr['basin_id']==ii].loc[:, ['pr', 'srad', 'tmax', 'tmin', 'vp', 'average_pr',\n",
        "       'average_q', 'average_tmax', 'average_tmin']])\n",
        "  temp_yy=np.asarray(df_test_tr[df_test_tr['basin_id']==ii]['q']).reshape((-1,1))\n",
        "  xx_,yy_=split_sequence_multi_train(temp_xx,temp_yy,365,0,mode='seq')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  y_out_tr=yy_[:,0].reshape((-1,1))\n",
        "  y_pred_tr=model_.predict(xx_)\n",
        "  y_m_out_tr=y_pred_tr.reshape((-1,1))\n",
        "\n",
        "  y_out_tr=y_out_tr*std_q+mean_q\n",
        "  y_m_out_tr=y_m_out_tr*std_q+mean_q\n",
        "\n",
        "  df_y_out_tr=pd.DataFrame(y_out_tr)\n",
        "  df_y_m_out_tr=pd.DataFrame(y_m_out_tr)\n",
        "  df_y_out_tr.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/ObsLSTMFine%d_sim.csv'%(ii), index=False)\n",
        "  df_y_m_out_tr.to_csv('/content/drive/MyDrive/PHIMP/Forecasts_CAMELS/LSTMFine%d_sim.csv'%(ii), index=False)\n",
        "\n",
        "\n",
        "\n",
        "  nse_scores_tr[list_diff.index(ii)]=nash_sutcliffe_error(y_out_tr,y_m_out_tr)\n",
        "  kge_scores_tr[list_diff.index(ii)]=KGE(y_m_out_tr,y_out_tr)\n",
        "  #print(nse_scores_tr[list_.index(ii)])\n",
        "\n",
        "\n",
        "nse_scores_tr_df = pd.DataFrame(nse_scores_tr,columns=['NSE'])\n",
        "nse_scores_tr_df.to_csv('/content/drive/MyDrive/PHIMP/LSTMFine.csv', index=False)\n",
        "\n",
        "kge_scores_tr_df = pd.DataFrame(kge_scores_tr,columns=['KGE'])\n",
        "kge_scores_tr_df.to_csv('/content/drive/MyDrive/PHIMP/LSTMFine_kge.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3JsqQjSjK_G",
        "outputId": "3704dbff-72b6-4c1b-cfe4-a5d00f00a50c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.71539226])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(nse_scores_tr,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXi4UayvjQEV",
        "outputId": "0d2a41b3-51d7-4b6b-fb66-d2be6cf223ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7379746638970837"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(kge_scores_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "qHHZJoVgIGa8",
        "outputId": "a9be62d2-fe73-4286-eed0-02859a5a76df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">365</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">272,384</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m365\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m272,384\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m257\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">272,641</span> (1.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m272,641\u001b[0m (1.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">272,641</span> (1.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m272,641\u001b[0m (1.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "32i_S8cFIIfc",
        "outputId": "9a650c17-9d42-48d9-eb51-fc0c30c712b6"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'summary'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-8e34cebf8c46>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'summary'"
          ]
        }
      ],
      "source": [
        "model_.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}